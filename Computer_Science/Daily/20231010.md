### HDFS
- 대용량 분산 데이터 저장
# spark
- 대용량 데이터를 처리하는 인메모리 고속 처리 엔진
- 클러스터 환경에서 병렬 처리
- 나온 이유 : 데이터를 저장하는 비용은 저렴해졌다. 하지만 데이터를 처리해야할 양이 많아졌다. 기존의 것들은 너무 느려서 빠른 대용량 데이터 처리가 필요했다!
- WORM(Write One Read Many) 
### ubuntu에 spark 설치
1. java 설치(apache spark는 java로 개발되었으므로) 및 환경변수 설정
2. apache spark 다운로드
3. spark 환경변수 설정(root로 설치했어도 각 개인 사용자 환경 변수에도 spark 환경 변수 설정한다. 환경 변수 설정은 사용자별로 다르다.)
4. 가상환경 만들어서 pip install pyspark

``` python
from pyspark import SparkContext,SparkConf
conf = SparkConf().setAppName("WordCount")
sc = SparkContext(conf=conf)
text_file=sc.textFile("example.txt")
# 각 줄을 공백으로 단어를 분할하고, 단어를 카운터한다. 
# .flatMap : 데이터를 여러 파티션으로 나누고 각 파티션을 병렬로 처리
# .reduceByKey 단어의 출현 횟수를 계산하는 작업을 병렬로 수행함
word_counts=text_file.flatMap(lambda line:line.split(" "))\
.map(lambda word:(word,1))\
.reduceByKey(lambda a,b:a+b)

for word,count in word_counts.collect():
    print(f"{word}: {count}")

'''
Hello,: 20
PySpark: 80
is: 40
perform: 20
transformations: 20
analysis.: 20
use: 20
process: 20
large: 20
datasets: 20
efficiently.: 20
an: 20
: 1
a: 20
powerful: 20
tool: 20
for: 40
distributed: 20
data: 40
processing.: 20
allows: 20
you: 20
to: 40
and: 20
You: 20
...
file: 20
word: 20
count.: 20
'''

```