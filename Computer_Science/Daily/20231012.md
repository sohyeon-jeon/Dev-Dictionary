```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("example").getOrCreate()

```


```python
# 한개의 컬럼과 1000개의 로우 
# 클러스터 모드에서 예제를 실행하면 각 부분이 서로 다른 익스큐터에 할당됨
myRange=spark.range(1000).toDF("number")
```


```python
# 파티션 : 클러스터의 물리적 머신에 존재하는 로우의 집합
# 만약 파티션이 1개라면 다수의 익스큐터가 있어도 병렬성 1 /
#  수백 개의 파티션이 있더라도 익스큐터가 하나밖에 없으면 병렬성 1

# 스파크의 핵심 데이터 구조는 불변성이다.
# 만약 변경하려면 스파크에게 변경 방법을 알려줘야하는데, 이 명령을 트랜스포메이션이라고한다.
# 아래코드를 호출하기전까지는 수행되지는 않는다. 호출해야 됨.
divisBy2=myRange.where("number % 2 = 0")

# 좁은 트랜스포메이션 : 하나의 입력 파티션이 하나의 출력 파티션에 영향(1:1)
# 위의 코드 where 구문은 좁은 트랜스포메이션
# 넓은 트랜스포메이션 : 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침(1:N)


```


```python
# 지연 연산 : 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식
# 스파크는 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션 실행 계획을 세운다.
# 스파크는 코드를 실행하는 마지막 순간까지 대기하다가 원형 DATAFRAME 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일합니다.
# 스파크는 이 과정을 거치며 전체 데이터 흐름을 최적한다

# 실제 연산을 수행하려면 액션 명령을 내려야 함.
# 액션은 일련의 트랜스포메이션으로부터 결과를 계산하도록 지시하는 명령입니다.
divisBy2.count()

# 500이 출력됨
# 액션을 지정하면 스파크 잡이 시작됨
# 스파크 잡은 좁은 트랜스포메이션을 수행 후 파티션별로 레코드 수를 넓은 트랜스포이션 합니다.
# 그리고 각 언어에 적합한 네이티브 객체에 결과를 모읍니다.

```




    500




```python
# 데이터프레임에서 csv 파일을 읽어 로컬 배열이나 리스트로 변환하는 과정
flightData=spark\
.read\
.option("inferSchema","true")\
.option("header","true")\
.csv("data/2015-summary.csv")

# 스파크는 각 컬럼의 데이터타입을 추론하기위해 적은 양의 데이터를 읽는다.
# (로우의 수를 알 수 없다, 지연 연산이기 때문에)

```


```python
flightData.take(3)
```




    [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),
     Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),
     Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]




```python
flightData.sort("count").explain()
# sort는 단지 트랜스포메이션이기 때문에 수행하지는 않는다.
# explain 메서드를 호출하여 실행계획을 확인해보자.
```

    == Physical Plan ==
    AdaptiveSparkPlan isFinalPlan=false
    +- Sort [count#51 ASC NULLS FIRST], true, 0
       +- Exchange rangepartitioning(count#51 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=195]
          +- FileScan csv [DEST_COUNTRY_NAME#49,ORIGIN_COUNTRY_NAME#50,count#51] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/BiO/home/shjeon/spark/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>
    
    



```python
# sort는 넓은 트랜스포메이션
# 스파크는 기본적으로 셔플 수행 시 200개의 셔플 파티션을 생성하는데
# 출력 파티션을 줄이기 위해 5로 설정
spark.conf.set("spark.sql.shuffle.partitions","5")
flightData.sort("count").take(2)
```




    [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),
     Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]




```python
# 데이터프레임을 테이블이나 뷰로 만든다.
flightData.createOrReplaceTempView("flight_data")
```


```python
sqlWay=spark.sql("""
SELECT DEST_COUNTRY_NAME,count(1)
FROM flight_data
GROUP BY DEST_COUNTRY_NAME
""")

dataFrameWay=flightData\
.groupBy("DEST_COUNTRY_NAME")\
.count()

sqlWay.explain()
dataFrameWay.explain()

```

    == Physical Plan ==
    AdaptiveSparkPlan isFinalPlan=false
    +- HashAggregate(keys=[DEST_COUNTRY_NAME#49], functions=[count(1)])
       +- Exchange hashpartitioning(DEST_COUNTRY_NAME#49, 5), ENSURE_REQUIREMENTS, [plan_id=229]
          +- HashAggregate(keys=[DEST_COUNTRY_NAME#49], functions=[partial_count(1)])
             +- FileScan csv [DEST_COUNTRY_NAME#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/BiO/home/shjeon/spark/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
    
    
    == Physical Plan ==
    AdaptiveSparkPlan isFinalPlan=false
    +- HashAggregate(keys=[DEST_COUNTRY_NAME#49], functions=[count(1)])
       +- Exchange hashpartitioning(DEST_COUNTRY_NAME#49, 5), ENSURE_REQUIREMENTS, [plan_id=242]
          +- HashAggregate(keys=[DEST_COUNTRY_NAME#49], functions=[partial_count(1)])
             +- FileScan csv [DEST_COUNTRY_NAME#49] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/BiO/home/shjeon/spark/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
    
    



```python
# 370002
spark.sql("SELECT max(count) from flight_data").take(1)

from pyspark.sql.functions import max
flightData.select(max("count")).take(1)

```




    [Row(max(count)=370002)]




```python
# 상위 5개 도착 국가
maxSql=spark.sql("""
SELECT DEST_COUNTRY_NAME,sum(count) as destination_total
FROM flight_data
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count) DESC
LIMIT 5
""")

maxSql.show()
```

    +-----------------+-----------------+
    |DEST_COUNTRY_NAME|destination_total|
    +-----------------+-----------------+
    |    United States|           411352|
    |           Canada|             8399|
    |           Mexico|             7140|
    |   United Kingdom|             2025|
    |            Japan|             1548|
    +-----------------+-----------------+
    



```python
from pyspark.sql.functions import desc

flightData\
.groupBy("DEST_COUNTRY_NAME")\
.sum("count")\
.withColumnRenamed("sum(count)","destination_total")\
.sort(desc("destination_total"))\
.limit(5)\
.show()

# show()
# 마지막 단계에서 액션을 수행한다. 이 때 데이터프레임 결과를 모으는 프로세스를 시작.
# 처리가 끝나면 리스트나 배열을 만든다.
```

    +-----------------+-----------------+
    |DEST_COUNTRY_NAME|destination_total|
    +-----------------+-----------------+
    |    United States|           411352|
    |           Canada|             8399|
    |           Mexico|             7140|
    |   United Kingdom|             2025|
    |            Japan|             1548|
    +-----------------+-----------------+
    

